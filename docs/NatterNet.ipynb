{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxY0AjT7bZ_6"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
        "# tell me a fun fact about space\n",
        "class BlenderBot:\n",
        "    def __init__(self, model_name=\"facebook/blenderbot-400M-distill\"):\n",
        "        self.tokenizer = BlenderbotTokenizer.from_pretrained(model_name)\n",
        "        self.model = BlenderbotForConditionalGeneration.from_pretrained(model_name)\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.model.to(self.device)\n",
        "\n",
        "    def get_response(self, user_input):\n",
        "        # Encode the user's input\n",
        "        input_ids = self.tokenizer([user_input], return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(self.device)\n",
        "\n",
        "        # Generate a response from the model\n",
        "        with torch.no_grad():\n",
        "            response_ids = self.model.generate(input_ids, max_length=512, num_return_sequences=1)\n",
        "\n",
        "        # Decode the response\n",
        "        response = self.tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "MNs3HI_9eLBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_with_bot():\n",
        "    bot = BlenderBot()\n",
        "\n",
        "    print(\"Bot: Hi! How can I assist you today?\")\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
        "            print(\"Bot: Goodbye!\")\n",
        "            break\n",
        "\n",
        "        bot_response = bot.get_response(user_input)\n",
        "        print(f\"Bot: {bot_response}\")"
      ],
      "metadata": {
        "id": "nPDU5kLp4lHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_with_bot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kw5fRLDbhZH",
        "outputId": "d95c56a1-7bd7-4b28-b464-ed09891f5441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: Hi! How can I assist you today?\n",
            "You: Can you explain the theory of relativity?\n",
            "Bot:  Relativity is the idea of the existence of a higher level of existence in the universe.\n",
            "You: Describe the plot of Shakespeare's 'Romeo and Juliet'\n",
            "Bot:  I love the story of the tragedy \"The Odyssey\" by Shakespare. Have you ever read it?\n",
            "You: What's your opinion on artificial intelligence?\n",
            "Bot:  I don't know much about it, but I do know that it has been used in research since the 1980s.\n",
            "You: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UbbfdROCcSqm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}